{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CUDA devices: 1\n",
      "LOCAL RANK=0\n",
      "NUM GPUS=1\n",
      "NODE=0\n",
      "GLOBAL RANK=0\n",
      "WORLD_SIZE=1\n",
      "\n",
      "__CONFIG__\n",
      "model_name = ridge_CLIPbase\n",
      "data_path = /teamspace/studios/this_studio/nsd\n",
      "clip_model_name = ViT-B/32\n",
      "batch_size = 128\n",
      "wandb_log = True\n",
      "ckpt_interval = 10\n",
      "ckpt_saving = False\n",
      "seed = 0\n",
      "max_lr = 0.0003\n",
      "num_epochs = 30\n",
      "temperature = 0.006\n",
      "\n",
      "\n",
      "device = cuda distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Standard Python libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Scientific computing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "# Deep learning frameworks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Custom utilities\n",
    "import utils\n",
    "\n",
    "# Progress bar library\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Interactive environment setup (only if running interactively)\n",
    "if utils.is_interactive():\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "# GPU configuration and setup\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True  # fixes Conv3D if used\n",
    "\n",
    "# Multi-GPU configuration\n",
    "device_count = torch.cuda.device_count()\n",
    "print(f\"Number of available CUDA devices: {device_count}\")\n",
    "\n",
    "local_rank = os.getenv('LOCAL_RANK')\n",
    "local_rank = 0 if local_rank is None else int(local_rank)\n",
    "print(f\"LOCAL RANK={local_rank}\")\n",
    "\n",
    "num_devices = os.getenv('NUM_GPUS')\n",
    "num_devices = 1 if num_devices is None else int(num_devices)\n",
    "print(f\"NUM GPUS={num_devices}\")\n",
    "distributed = True if num_devices > 1 else False\n",
    "if distributed: assert device_count == num_devices\n",
    "\n",
    "node = os.getenv('SLURM_NODEID')\n",
    "node = 0 if node is None else int(node)\n",
    "print(f\"NODE={node}\")\n",
    "\n",
    "global_rank = os.getenv('RANK')\n",
    "global_rank = 0 if global_rank is None else int(global_rank)\n",
    "print(f\"GLOBAL RANK={global_rank}\")\n",
    "\n",
    "world_size = os.getenv('WORLD_SIZE')\n",
    "world_size = 1 if world_size is None else int(world_size)\n",
    "print(f\"WORLD_SIZE={world_size}\")\n",
    "\n",
    "# Load parameters from yaml config\n",
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# Create global variables from the config\n",
    "print(\"\\n__CONFIG__\")\n",
    "for attribute_name in config.keys():\n",
    "    print(f\"{attribute_name} = {config[attribute_name]}\")\n",
    "    globals()[attribute_name] = config[f'{attribute_name}']\n",
    "print(\"\\n\")\n",
    "\n",
    "# Set up data type and device\n",
    "data_type = torch.float32  # change depending on your mixed_precision\n",
    "global_batch_size = batch_size * world_size\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# Print setup information\n",
    "print(\"device =\", device, \"distributed =\", distributed, \"num_devices =\", num_devices,\n",
    "      \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "\n",
    "# Seed all random functions\n",
    "utils.seed_everything(seed + global_rank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "source": [
    "#### Load all voxels for a given NSD subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c0928ed-910e-4334-9ce0-96245a364546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voxels (30000, 15724)\n"
     ]
    }
   ],
   "source": [
    "subject_id = 1\n",
    "f = h5py.File(f'{data_path}/betas_all_subj0{subject_id}_fp32_renorm.hdf5', 'r')\n",
    "voxels = f['betas'][:] # preloading all voxels for given subject\n",
    "print(\"voxels\", voxels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ab44c6-761e-4609-a4bb-08ac340c88ad",
   "metadata": {},
   "source": [
    "#### Load all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be67a411-45f0-4b2e-9d6a-2d85017a6a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images (73000, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'][:] # preloading all images for easy indexing\n",
    "print(\"images\", images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e17a8-f326-45eb-9055-dd36ea1843bc",
   "metadata": {},
   "source": [
    "#### Pair voxels to images/image captions and define train/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f142c37b-fddf-4ad4-a75a-6634f942bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared1000 = np.load(f\"{data_path}/shared1000.npy\")\n",
    "\n",
    "# image train/test split\n",
    "file_path = f\"{data_path}/COCO_73k_subj_indices.hdf5\"\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    data = {key: f[key][()] for key in f.keys()}\n",
    "\n",
    "image_indices = data[\"subj01\"]\n",
    "is_shared = shared1000[image_indices]\n",
    "\n",
    "image_indices_train = image_indices[~is_shared]\n",
    "image_indices_test = image_indices[is_shared]\n",
    "\n",
    "images_train = torch.tensor(images[image_indices_train]).float() # (27000, 3, 224, 224)\n",
    "images_test = torch.tensor(images[image_indices_test]).float() # (3000, 3, 224, 224)\n",
    "\n",
    "print(\"images_train\", images_train.shape)\n",
    "print(\"images_test\", images_test.shape)\n",
    "\n",
    "# load image captions\n",
    "annots = np.load(f'{data_path}/COCO_73k_annots_curated.npy')\n",
    "print(\"annots\", annots.shape)\n",
    "\n",
    "# check that annotations line up with image order\n",
    "display(utils.torch_to_Image(images_train[:1]))\n",
    "print(utils.get_annotations(annots[image_indices_train][:1],random=False))\n",
    "\n",
    "# voxel train/test split\n",
    "shared_voxel_indices = shared1000[data[\"subj01\"]]\n",
    "voxels_train = torch.tensor(voxels[~shared_voxel_indices]) # (27000, 15724)\n",
    "voxels_test = torch.tensor(voxels[shared_voxel_indices]) # (3000, 15724)\n",
    "\n",
    "num_voxels = voxels_train.shape[-1]\n",
    "\n",
    "print(\"voxels_train\", voxels_train.shape)\n",
    "print(\"voxels_test\", voxels_test.shape)\n",
    "\n",
    "del images # free up memory for images we arent using"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4650d2a4-b7ce-4922-a62f-d7c370eb39bd",
   "metadata": {},
   "source": [
    "#### Create torch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2a707f-0c38-4e08-ac5f-660c1208c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.utils.data.TensorDataset(images_train, voxels_train, torch.tensor(image_indices_train))\n",
    "test_data = torch.utils.data.TensorDataset(images_test, voxels_test, torch.tensor(image_indices_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290db274-c868-4236-9d51-58f4ff089a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=300, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "num_samples_per_epoch= len(train_dl.dataset)\n",
    "print(\"num_samples_per_epoch\", num_samples_per_epoch)\n",
    "num_iterations_per_epoch = len(train_dl)\n",
    "print(\"num_iterations_per_epoch\", num_iterations_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "source": [
    "## Create/load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "clip_model, _ = clip.load(clip_model_name, device=device)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                         std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "def clip_img_embedder(image):\n",
    "    preproc_img = preprocess(image)\n",
    "    return clip_model.encode_image(preproc_img)\n",
    "if clip_model_name==\"ViT-L/14\":\n",
    "    clip_emb_dim = 768\n",
    "elif clip_model_name==\"ViT-B/32\":\n",
    "    clip_emb_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038a5d61-4769-40b9-a004-f4e7b5b38bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression(torch.nn.Module):\n",
    "    def __init__(self, input_size, final_out): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.linears = torch.nn.Linear(input_size, final_out)\n",
    "    def forward(self, x):\n",
    "        out = self.linears(x)\n",
    "        return out\n",
    "\n",
    "model = RidgeRegression(num_voxels, final_out=clip_emb_dim)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test on subject 1 with fake data\n",
    "b = torch.randn((2,num_voxels))\n",
    "print(b.shape, model(b).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25271a-2209-400c-8026-df3b8ddc1eef",
   "metadata": {},
   "source": [
    "## Setup optimizer / lr / ckpt saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters()], 'weight_decay': 1e-2},\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "print(\"total_steps\", total_steps)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=max_lr,\n",
    "    total_steps=total_steps,\n",
    "    final_div_factor=1000,\n",
    "    last_epoch=-1, pct_start=2/num_epochs\n",
    ")\n",
    "\n",
    "outdir = os.path.abspath(f'../ckpts/{model_name}')\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': unwrapped_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        }, ckpt_path)\n",
    "    print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "\n",
    "def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True,outdir=outdir): \n",
    "    print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    model.load_state_dict(state_dict, strict=strict)\n",
    "    if load_epoch:\n",
    "        globals()[\"epoch\"] = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "    if load_optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if load_lr:\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    del checkpoint\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "## Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'brain_filtering'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_params\": num_params,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"never\",\n",
    "    ) # would need to make modifications to training code to allow resume=\"allow\"\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be0d5f-3e94-4612-9373-61b53d836393",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1000, disable=(local_rank!=0))\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    fwd_percent_correct, test_fwd_percent_correct = 0., 0.\n",
    "    bwd_percent_correct, test_bwd_percent_correct = 0., 0.\n",
    "    \n",
    "    for train_i, data in enumerate(train_dl):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        image, voxel, idx = data\n",
    "        image = image.to(device)\n",
    "        voxel = voxel.to(device)\n",
    "        # annot = utils.get_annotations(annots[idx],random=False)\n",
    "        # annot = clip.tokenize(annot).to(device)\n",
    "\n",
    "        clip_brain = model(voxel)\n",
    "        clip_image = clip_img_embedder(image).to(clip_brain.dtype)\n",
    "        # clip_text = clip_model.encode_text(annot).to(clip_image.dtype) \n",
    "        \n",
    "        labels = torch.arange(len(clip_image)).to(device)\n",
    "        \n",
    "        clip_image = nn.functional.normalize(clip_image, dim=-1)\n",
    "        # clip_text = nn.functional.normalize(clip_text, dim=-1)\n",
    "        clip_brain = nn.functional.normalize(clip_brain, dim=-1)\n",
    "\n",
    "        logits_image_brain = (clip_image @ clip_brain.T) / temperature\n",
    "        loss_image_brain = nn.functional.cross_entropy(logits_image_brain, labels)\n",
    "        logits_brain_image = (clip_brain @ clip_image.T) / temperature\n",
    "        loss_brain_image = nn.functional.cross_entropy(logits_brain_image, labels)\n",
    "\n",
    "        loss = loss_image_brain + loss_brain_image\n",
    "        \n",
    "        fwd_ret = utils.topk(utils.batchwise_cosine_similarity(clip_brain, clip_image), labels, k=1).item()\n",
    "        bwd_ret = utils.topk(utils.batchwise_cosine_similarity(clip_image, clip_brain), labels, k=1).item()\n",
    "            \n",
    "        fwd_percent_correct += fwd_ret\n",
    "        bwd_percent_correct += bwd_ret\n",
    "    \n",
    "        utils.check_loss(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        losses.append(loss.item())\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "        lr_scheduler.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad(): \n",
    "        for test_i, test_data in enumerate(test_dl): \n",
    "            image, voxel, idx = test_data\n",
    "            image = image.to(device)\n",
    "            voxel = voxel.to(device)\n",
    "            annot = utils.get_annotations(annots[idx],random=False)\n",
    "            annot = clip.tokenize(annot).to(device)\n",
    "            \n",
    "            clip_brain = model(voxel)\n",
    "            clip_image = clip_img_embedder(image).to(clip_brain.dtype)\n",
    "            # clip_text = clip_model.encode_text(annot).to(clip_image.dtype)\n",
    "            \n",
    "            labels = torch.arange(len(clip_image)).to(device)\n",
    "\n",
    "            clip_image = nn.functional.normalize(clip_image, dim=-1)\n",
    "            # clip_text = nn.functional.normalize(clip_text, dim=-1)\n",
    "            clip_brain = nn.functional.normalize(clip_brain, dim=-1)\n",
    "    \n",
    "            logits_image_brain = (clip_image @ clip_brain.T) / temperature\n",
    "            loss_image_brain = nn.functional.cross_entropy(logits_image_brain, labels)\n",
    "            logits_brain_image = (clip_brain @ clip_image.T) / temperature\n",
    "            loss_brain_image = nn.functional.cross_entropy(logits_brain_image, labels)\n",
    "    \n",
    "            loss = loss_image_brain + loss_brain_image\n",
    "            \n",
    "            fwd_ret = utils.topk(utils.batchwise_cosine_similarity(clip_brain, clip_image), labels, k=1).item()\n",
    "            bwd_ret = utils.topk(utils.batchwise_cosine_similarity(clip_image, clip_brain), labels, k=1).item()\n",
    "                \n",
    "            test_fwd_percent_correct += fwd_ret\n",
    "            test_bwd_percent_correct += bwd_ret\n",
    "            \n",
    "            utils.check_loss(loss)                \n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "        logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "            \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "            \"train/lr\": lrs[-1],\n",
    "            \"train/num_steps\": len(losses),\n",
    "            \"test/num_steps\": len(test_losses),\n",
    "            \"train/fwd_percent_correct\": fwd_percent_correct / (train_i + 1),\n",
    "            \"test/test_fwd_percent_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "            \"train/bwd_percent_correct\": bwd_percent_correct / (train_i + 1),\n",
    "            \"test/test_bwd_percent_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "            }\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        \n",
    "        if wandb_log: wandb.log(logs)\n",
    "                \n",
    "        # Save model checkpoint and reconstruct\n",
    "        if (ckpt_saving) and (epoch % ckpt_interval == 0): \n",
    "            save_ckpt(f'last')\n",
    "    \n",
    "        # wait for other GPUs to catch up if needed\n",
    "        if distributed: \n",
    "            dist.barrier()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "\n",
    "if ckpt_saving: \n",
    "    save_ckpt(f'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e81ae3-171f-40ad-a3e8-24bee4472325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(test_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e01c0-b021-4bd6-8a03-4880f461f04b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a4f33-242d-4e1b-a3bd-44aaf0048927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindeye",
   "language": "python",
   "name": "mindeye"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
